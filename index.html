<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>MASE</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Bootstrap icons-->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css" rel="stylesheet" type="text/css" />
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body>
        <!-- Navigation-->
        <nav class="navbar navbar-light bg-light static-top">
            <div class="container">
                <a class="navbar-brand" href="#!">Multi-Aspect Subtask Evaluation (MASE)</a>
            </div>
        </nav>

        <section class="showcase">
            <div class="container-fluid p-0">
                <div class="row g-0">
                    <div class="col-lg-12 order-lg-1 my-auto showcase-text">
                        <h2>Multi-Aspect Subtask Evaluation (MASE):</h2>
                        <h4>Rethinking End-to-End Evaluation of Decomposable Tasks</h4>
                  			<a class="btn btn-primary" href="#signup">Paper</a>
                  			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                  			<a class="btn btn-primary" href="https://github.com/maseEval/mase">Code</a>
                  			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        <a class="btn btn-primary" href="bibtex.txt">Bibtex</a>
                  			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        <a class="btn btn-primary" href="https://github.com/maseEval/mase/tree/main/slu_splits">Splits</a>
                        <br>
                        <br>
                        <p class="lead mb-0">To be presented at InterSpeech 2021.</p>
                    </div>
                </div>
            </div>
        </section>
        
        <!-- Icons Grid-->
        <section class="features-icons bg-light text-center">
            <section id="about" class="container text-center">
            <div class="row">
                <div class="lead">
                  <br><br>
                    <h2>Rethinking End-to-End Evaluation of Decomposable Tasks</h2>
                <h4> <i>To appear at InterSpeech 2021</i></h4>
                  </div>
            </div>
            </section>
            <br>
<div class="container">
    
    <div class="row">
        <div class="lead">

            <p>
            Decomposable tasks are complex and comprise of an hierarchy of sub-tasks.  Spoken intent prediction, for example, combines automatic speech recognition and natural language understanding. Existing benchmarks, however, typically hold out examples for  only  the  surface-level  sub-task.   As  a  result,  models with similar performance on these benchmarks may have unobserved  performance  differences  on  the  other  sub-tasks.   To allow insightful comparisons between competitive end-to-end architectures, we propose a framework to construct robust test sets using coordinate ascent over sub-task specific utility functions. Given  a  dataset  for  a  decomposable  task,  our  method optimally creates a test set for each sub-task to  individually  assess  sub-components of the end-to-end model. Using spoken language understanding as a case study, we generate new splits for the Fluent Speech Commands and Snips SmartLights datasets. Each split has two test sets:  one with held-out utterances assessing natural language understanding abilities, and one with held-out speakers to test speech processing skills.  Our splits identify performance gaps up to 10% between end-to-end systems that were within 1% of each other on the original test sets.  These performance gaps allow more realistic and actionable comparisons between different architectures, driving future model development. We release our splits and tools for the community.
               
            <p></p>

        <p> 

        </p></div>

    </div>
</div>
            
        </section>
    </body>
</html>
